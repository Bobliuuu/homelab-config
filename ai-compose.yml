version: "3.9"

name: ai-stack

networks:
  ai_net:

volumes:
  ollama_data:
  opensearch_data:
  qdrant_data:
  pg_data:
  redis_data:

services:
  # --- LLM runtime ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama:Z
    # If you have NVIDIA GPU + proper OCI hooks, uncomment (host-dependent):
    # device_requests:
    #   - driver: nvidia
    #     count: -1
    #     capabilities: ["gpu"]

  # --- Simple UI for Ollama ---
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama

  # --- Vector DB (great with LlamaIndex) ---
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage:Z

  # --- Search / analytics (OpenSearch) ---
  opensearch:
    image: opensearchproject/opensearch:2
    container_name: opensearch
    restart: unless-stopped
    networks: [ai_net]
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
      - plugins.security.disabled=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - opensearch_data:/usr/share/opensearch/data:Z

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2
    container_name: opensearch-dashboards
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "5601:5601"
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    depends_on:
      - opensearch

  # --- Common backing services (optional but useful) ---
  postgres:
    image: postgres:16
    container_name: ai-postgres
    restart: unless-stopped
    networks: [ai_net]
    environment:
      - POSTGRES_DB=ai
      - POSTGRES_USER=ai
      - POSTGRES_PASSWORD=ai_change_me
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data:Z

  redis:
    image: redis:7
    container_name: ai-redis
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "6379:6379"
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data:Z

  # --- LlamaIndex "service" (you provide app) ---
  # LlamaIndex is a Python library, so you normally run it inside *your own* API.
  # Put a Dockerfile + app/ folder next to this YAML, then build it.
  llamaindex-api:
    build:
      context: ./llamaindex-api
      dockerfile: Dockerfile
    container_name: llamaindex-api
    restart: unless-stopped
    networks: [ai_net]
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_URL=http://qdrant:6333
      - OPENSEARCH_URL=http://opensearch:9200
      - DATABASE_URL=postgresql://ai:ai_change_me@postgres:5432/ai
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - ollama
      - qdrant
      - opensearch
      - postgres
      - redis
